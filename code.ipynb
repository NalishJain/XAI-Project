{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fee41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install google-generativeai\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "# !unzip tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4b108c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import time\n",
    "import random\n",
    "import google.generativeai as genai\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c58d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concept_list(class_names):\n",
    "    \n",
    "    concept_set = set()\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')  \n",
    "    count = 0\n",
    "    \n",
    "    for cname in tqdm(class_names, desc=\"Generating concepts\"):\n",
    "        prompts = [\n",
    "            f\"List exactly 5-10 of the most important features for recognizing something as a {cname}. Format as a simple list with one feature per line, no bullets, numbering, or explanations.\",\n",
    "            f\"List exactly 5-10 things most commonly seen around a {cname}. Format as a simple list with one item per line, no bullets, numbering, or explanations.\",\n",
    "            f\"List exactly 3-5 superclasses or categories for the word {cname}. Format as a simple list with one category per line, no bullets, numbering, or explanations.\"\n",
    "        ]\n",
    "        \n",
    "        for prompt_id, prompt in enumerate(prompts):\n",
    "            max_retries = 5\n",
    "            retry_count = 0\n",
    "            retry_delay = 2  \n",
    "            \n",
    "            print(\"prompt id: \", prompt_id)\n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    response = model.generate_content(\n",
    "                        prompt,\n",
    "                        generation_config={\n",
    "                            'temperature': 0.2,  # Lower temperature for more consistent formatting\n",
    "                            'top_p': 0.95,\n",
    "                            'top_k': 40,\n",
    "                            'max_output_tokens': 300,\n",
    "                        }\n",
    "                    )\n",
    "                    content = response.text\n",
    "                    lines = content.split('\\n')\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        # Skip empty lines and header/instructional text\n",
    "                        if not line.strip() or \"list\" in line.lower() or \"feature\" in line.lower() or \"category\" in line.lower():\n",
    "                            continue\n",
    "                            \n",
    "                        cleaned = line.strip(\" .•-*0123456789:()[]{}\\\"\\',\").lower()\n",
    "                        print(\"---\")\n",
    "                        print(cleaned)\n",
    "                        print(\"-.-\")\n",
    "\n",
    "                        if cleaned and len(cleaned) > 2 and cname.lower() not in cleaned:\n",
    "                            concept_set.add(cleaned)\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    print(f\"Gemini API error: {error_msg}\")\n",
    "                    \n",
    "                    # Check if it's a rate limit error (429)\n",
    "                    if \"429\" in error_msg:\n",
    "                        # Extract retry delay from error if available\n",
    "                        import re\n",
    "                        delay_match = re.search(r'retry_delay \\{\\s*seconds: (\\d+)', error_msg)\n",
    "                        \n",
    "                        if delay_match:\n",
    "                            # Use the suggested delay from the API\n",
    "                            retry_seconds = int(delay_match.group(1))\n",
    "                            retry_seconds += random.uniform(0, 2)  # Add small random jitter\n",
    "                        else:\n",
    "                            # Exponential backoff with jitter\n",
    "                            retry_seconds = retry_delay + random.uniform(0, retry_delay * 0.1)\n",
    "                            retry_delay *= 2  # Double the delay for next retry\n",
    "                            \n",
    "                        print(f\"Rate limited. Retrying in {retry_seconds:.1f} seconds...\")\n",
    "                        time.sleep(retry_seconds)\n",
    "                        retry_count += 1\n",
    "                    else:\n",
    "                        # For non-rate-limit errors, just print and continue\n",
    "                        print(f\"Error (not retrying): {error_msg}\")\n",
    "                        break\n",
    "            \n",
    "            # Add a small delay between successful requests to avoid hitting rate limits\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Return concepts that are between 2 and 5 words long\n",
    "    return sorted([c for c in concept_set if 2 <= len(c.split()) <= 5])\n",
    "\n",
    "# Decode WNIDs to human-readable class names\n",
    "def decode_wnid(wnid):\n",
    "    synset = wn.synset_from_pos_and_offset(wnid[0], int(wnid[1:]))\n",
    "    return synset.name().split('.')[0].replace('_', ' ')\n",
    "\n",
    "def draw_red_circle(image, center, radius):\n",
    "    img = image.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    x, y = center\n",
    "    draw.ellipse((x - radius, y - radius, x + radius, y + radius), outline=\"red\", width=2)\n",
    "    return img\n",
    "\n",
    "def compute_spatial_similarity_matrix(images, concept_list, model, preprocess, device,\n",
    "                                      grid_size=(7, 7), radius=32):\n",
    "    model.eval()\n",
    "    H̃, W̃ = grid_size\n",
    "    P = torch.zeros((len(images), len(concept_list), H̃, W̃))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Precompute concept embeddings\n",
    "        text_tokens = clip.tokenize(concept_list).to(device)\n",
    "        concept_embeddings = model.encode_text(text_tokens)\n",
    "        concept_embeddings = concept_embeddings / concept_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "        for n, image in enumerate(images):\n",
    "            print(\"--\", n)\n",
    "            width, height = image.size\n",
    "            dH = height // (H̃ + 1)\n",
    "            dW = width // (W̃ + 1)\n",
    "\n",
    "            for h in range(H̃):\n",
    "                for w in range(W̃):\n",
    "                    cx = (w + 1) * dW\n",
    "                    cy = (h + 1) * dH\n",
    "                    prompted_img = draw_red_circle(image, (cx, cy), radius)\n",
    "                    input_tensor = preprocess(prompted_img).unsqueeze(0).to(device)\n",
    "\n",
    "                    image_embedding = model.encode_image(input_tensor)\n",
    "                    image_embedding = image_embedding / image_embedding.norm(dim=1, keepdim=True)\n",
    "\n",
    "                    sim = (image_embedding @ concept_embeddings.T).squeeze(0)  # (M,)\n",
    "                    P[n, :, h, w] = sim\n",
    "\n",
    "    return P  # Shape: [N, M, H̃, W̃]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder('tiny-imagenet-200/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "val_dataset = datasets.ImageFolder('tiny-imagenet-200/val', transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "idx_to_wnid = {v: k for k, v in train_dataset.class_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19138c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'n01443537',\n",
       " 1: 'n01629819',\n",
       " 2: 'n01641577',\n",
       " 3: 'n01644900',\n",
       " 4: 'n01698640',\n",
       " 5: 'n01742172',\n",
       " 6: 'n01768244',\n",
       " 7: 'n01770393',\n",
       " 8: 'n01774384',\n",
       " 9: 'n01774750',\n",
       " 10: 'n01784675',\n",
       " 11: 'n01855672',\n",
       " 12: 'n01882714',\n",
       " 13: 'n01910747',\n",
       " 14: 'n01917289',\n",
       " 15: 'n01944390',\n",
       " 16: 'n01945685',\n",
       " 17: 'n01950731',\n",
       " 18: 'n01983481',\n",
       " 19: 'n01984695',\n",
       " 20: 'n02002724',\n",
       " 21: 'n02056570',\n",
       " 22: 'n02058221',\n",
       " 23: 'n02074367',\n",
       " 24: 'n02085620',\n",
       " 25: 'n02094433',\n",
       " 26: 'n02099601',\n",
       " 27: 'n02099712',\n",
       " 28: 'n02106662',\n",
       " 29: 'n02113799',\n",
       " 30: 'n02123045',\n",
       " 31: 'n02123394',\n",
       " 32: 'n02124075',\n",
       " 33: 'n02125311',\n",
       " 34: 'n02129165',\n",
       " 35: 'n02132136',\n",
       " 36: 'n02165456',\n",
       " 37: 'n02190166',\n",
       " 38: 'n02206856',\n",
       " 39: 'n02226429',\n",
       " 40: 'n02231487',\n",
       " 41: 'n02233338',\n",
       " 42: 'n02236044',\n",
       " 43: 'n02268443',\n",
       " 44: 'n02279972',\n",
       " 45: 'n02281406',\n",
       " 46: 'n02321529',\n",
       " 47: 'n02364673',\n",
       " 48: 'n02395406',\n",
       " 49: 'n02403003',\n",
       " 50: 'n02410509',\n",
       " 51: 'n02415577',\n",
       " 52: 'n02423022',\n",
       " 53: 'n02437312',\n",
       " 54: 'n02480495',\n",
       " 55: 'n02481823',\n",
       " 56: 'n02486410',\n",
       " 57: 'n02504458',\n",
       " 58: 'n02509815',\n",
       " 59: 'n02666196',\n",
       " 60: 'n02669723',\n",
       " 61: 'n02699494',\n",
       " 62: 'n02730930',\n",
       " 63: 'n02769748',\n",
       " 64: 'n02788148',\n",
       " 65: 'n02791270',\n",
       " 66: 'n02793495',\n",
       " 67: 'n02795169',\n",
       " 68: 'n02802426',\n",
       " 69: 'n02808440',\n",
       " 70: 'n02814533',\n",
       " 71: 'n02814860',\n",
       " 72: 'n02815834',\n",
       " 73: 'n02823428',\n",
       " 74: 'n02837789',\n",
       " 75: 'n02841315',\n",
       " 76: 'n02843684',\n",
       " 77: 'n02883205',\n",
       " 78: 'n02892201',\n",
       " 79: 'n02906734',\n",
       " 80: 'n02909870',\n",
       " 81: 'n02917067',\n",
       " 82: 'n02927161',\n",
       " 83: 'n02948072',\n",
       " 84: 'n02950826',\n",
       " 85: 'n02963159',\n",
       " 86: 'n02977058',\n",
       " 87: 'n02988304',\n",
       " 88: 'n02999410',\n",
       " 89: 'n03014705',\n",
       " 90: 'n03026506',\n",
       " 91: 'n03042490',\n",
       " 92: 'n03085013',\n",
       " 93: 'n03089624',\n",
       " 94: 'n03100240',\n",
       " 95: 'n03126707',\n",
       " 96: 'n03160309',\n",
       " 97: 'n03179701',\n",
       " 98: 'n03201208',\n",
       " 99: 'n03250847',\n",
       " 100: 'n03255030',\n",
       " 101: 'n03355925',\n",
       " 102: 'n03388043',\n",
       " 103: 'n03393912',\n",
       " 104: 'n03400231',\n",
       " 105: 'n03404251',\n",
       " 106: 'n03424325',\n",
       " 107: 'n03444034',\n",
       " 108: 'n03447447',\n",
       " 109: 'n03544143',\n",
       " 110: 'n03584254',\n",
       " 111: 'n03599486',\n",
       " 112: 'n03617480',\n",
       " 113: 'n03637318',\n",
       " 114: 'n03649909',\n",
       " 115: 'n03662601',\n",
       " 116: 'n03670208',\n",
       " 117: 'n03706229',\n",
       " 118: 'n03733131',\n",
       " 119: 'n03763968',\n",
       " 120: 'n03770439',\n",
       " 121: 'n03796401',\n",
       " 122: 'n03804744',\n",
       " 123: 'n03814639',\n",
       " 124: 'n03837869',\n",
       " 125: 'n03838899',\n",
       " 126: 'n03854065',\n",
       " 127: 'n03891332',\n",
       " 128: 'n03902125',\n",
       " 129: 'n03930313',\n",
       " 130: 'n03937543',\n",
       " 131: 'n03970156',\n",
       " 132: 'n03976657',\n",
       " 133: 'n03977966',\n",
       " 134: 'n03980874',\n",
       " 135: 'n03983396',\n",
       " 136: 'n03992509',\n",
       " 137: 'n04008634',\n",
       " 138: 'n04023962',\n",
       " 139: 'n04067472',\n",
       " 140: 'n04070727',\n",
       " 141: 'n04074963',\n",
       " 142: 'n04099969',\n",
       " 143: 'n04118538',\n",
       " 144: 'n04133789',\n",
       " 145: 'n04146614',\n",
       " 146: 'n04149813',\n",
       " 147: 'n04179913',\n",
       " 148: 'n04251144',\n",
       " 149: 'n04254777',\n",
       " 150: 'n04259630',\n",
       " 151: 'n04265275',\n",
       " 152: 'n04275548',\n",
       " 153: 'n04285008',\n",
       " 154: 'n04311004',\n",
       " 155: 'n04328186',\n",
       " 156: 'n04356056',\n",
       " 157: 'n04366367',\n",
       " 158: 'n04371430',\n",
       " 159: 'n04376876',\n",
       " 160: 'n04398044',\n",
       " 161: 'n04399382',\n",
       " 162: 'n04417672',\n",
       " 163: 'n04456115',\n",
       " 164: 'n04465501',\n",
       " 165: 'n04486054',\n",
       " 166: 'n04487081',\n",
       " 167: 'n04501370',\n",
       " 168: 'n04507155',\n",
       " 169: 'n04532106',\n",
       " 170: 'n04532670',\n",
       " 171: 'n04540053',\n",
       " 172: 'n04560804',\n",
       " 173: 'n04562935',\n",
       " 174: 'n04596742',\n",
       " 175: 'n04597913',\n",
       " 176: 'n06596364',\n",
       " 177: 'n07579787',\n",
       " 178: 'n07583066',\n",
       " 179: 'n07614500',\n",
       " 180: 'n07615774',\n",
       " 181: 'n07695742',\n",
       " 182: 'n07711569',\n",
       " 183: 'n07715103',\n",
       " 184: 'n07720875',\n",
       " 185: 'n07734744',\n",
       " 186: 'n07747607',\n",
       " 187: 'n07749582',\n",
       " 188: 'n07753592',\n",
       " 189: 'n07768694',\n",
       " 190: 'n07871810',\n",
       " 191: 'n07873807',\n",
       " 192: 'n07875152',\n",
       " 193: 'n07920052',\n",
       " 194: 'n09193705',\n",
       " 195: 'n09246464',\n",
       " 196: 'n09256479',\n",
       " 197: 'n09332890',\n",
       " 198: 'n09428293',\n",
       " 199: 'n12267677'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_wnid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f6a91c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 335M/335M [01:10<00:00, 4.95MiB/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "grid_size = (7, 7)\n",
    "circle_radius = 32\n",
    "my_key = \"AIzaSyBiUwxyp8ASs_UgameBEwv5NgWUlTXLMWA\"\n",
    "genai.configure(api_key=my_key)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd7eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 200\n",
      "Exact class names: ['egyptian cat', 'reel', 'volleyball', 'rocking chair', 'lemon', 'bullfrog', 'basketball', 'cliff', 'espresso', 'plunger']\n"
     ]
    }
   ],
   "source": [
    "TINY_IMAGENET_ROOT = \"tiny-imagenet-200\"\n",
    "\n",
    "# Step 1: Read wnids.txt\n",
    "with open(os.path.join(TINY_IMAGENET_ROOT, 'wnids.txt'), 'r') as f:\n",
    "    wnids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(\"Total classes:\", len(wnids))\n",
    "class_names = [decode_wnid(wnid) for wnid in wnids]\n",
    "print(\"Exact class names:\", class_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9d9752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 images from 10 classes.\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "selected_images = []\n",
    "counts = {wnid: 0 for wnid in wnids[:10]}\n",
    "\n",
    "for path, label in train_dataset.samples:\n",
    "    # print(label)\n",
    "    wnid = idx_to_wnid[label]\n",
    "    if wnid in wnids[:10] and counts[wnid] < N:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        # img = pil_transform(img)\n",
    "        selected_images.append(img)\n",
    "        counts[wnid] += 1\n",
    "    if all(c >= N for c in counts.values()):\n",
    "        break\n",
    "\n",
    "# selected_images is now a list of PIL images for the first 10 classes\n",
    "print(f\"Collected {len(selected_images)} images from {len(wnids[:10])} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = generate_concept_list(class_names[:10])  \n",
    "print(\"Generated Concepts:\", concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = compute_spatial_similarity_matrix(\n",
    "    images=selected_images,\n",
    "    concept_list=concepts,  \n",
    "    model=model,\n",
    "    preprocess=preprocess,\n",
    "    device=device,\n",
    "    grid_size=(7, 7),\n",
    "    radius=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d642951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 87, 7, 7])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
