{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fee41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install google-generativeai\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "# !unzip tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b108c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import time\n",
    "import random\n",
    "import google.generativeai as genai\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c58d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concept_list(class_names):\n",
    "    \n",
    "    concept_set = set()\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')  \n",
    "    count = 0\n",
    "    \n",
    "    for cname in tqdm(class_names, desc=\"Generating concepts\"):\n",
    "        prompts = [\n",
    "            f\"List exactly 5-10 of the most important features for recognizing something as a {cname}. Format as a simple list with one feature per line, no bullets, numbering, or explanations.\",\n",
    "            f\"List exactly 5-10 things most commonly seen around a {cname}. Format as a simple list with one item per line, no bullets, numbering, or explanations.\",\n",
    "            f\"List exactly 3-5 superclasses or categories for the word {cname}. Format as a simple list with one category per line, no bullets, numbering, or explanations.\"\n",
    "        ]\n",
    "        \n",
    "        for prompt_id, prompt in enumerate(prompts):\n",
    "            max_retries = 5\n",
    "            retry_count = 0\n",
    "            retry_delay = 2  \n",
    "            \n",
    "            print(\"prompt id: \", prompt_id)\n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    response = model.generate_content(\n",
    "                        prompt,\n",
    "                        generation_config={\n",
    "                            'temperature': 0.2,  # Lower temperature for more consistent formatting\n",
    "                            'top_p': 0.95,\n",
    "                            'top_k': 40,\n",
    "                            'max_output_tokens': 300,\n",
    "                        }\n",
    "                    )\n",
    "                    content = response.text\n",
    "                    lines = content.split('\\n')\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        # Skip empty lines and header/instructional text\n",
    "                        if not line.strip() or \"list\" in line.lower() or \"feature\" in line.lower() or \"category\" in line.lower():\n",
    "                            continue\n",
    "                            \n",
    "                        cleaned = line.strip(\" .•-*0123456789:()[]{}\\\"\\',\").lower()\n",
    "                        print(\"---\")\n",
    "                        print(cleaned)\n",
    "                        print(\"-.-\")\n",
    "\n",
    "                        if cleaned and len(cleaned) > 2 and cname.lower() not in cleaned:\n",
    "                            concept_set.add(cleaned)\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    print(f\"Gemini API error: {error_msg}\")\n",
    "                    \n",
    "                    # Check if it's a rate limit error (429)\n",
    "                    if \"429\" in error_msg:\n",
    "                        # Extract retry delay from error if available\n",
    "                        import re\n",
    "                        delay_match = re.search(r'retry_delay \\{\\s*seconds: (\\d+)', error_msg)\n",
    "                        \n",
    "                        if delay_match:\n",
    "                            # Use the suggested delay from the API\n",
    "                            retry_seconds = int(delay_match.group(1))\n",
    "                            retry_seconds += random.uniform(0, 2)  # Add small random jitter\n",
    "                        else:\n",
    "                            # Exponential backoff with jitter\n",
    "                            retry_seconds = retry_delay + random.uniform(0, retry_delay * 0.1)\n",
    "                            retry_delay *= 2  # Double the delay for next retry\n",
    "                            \n",
    "                        print(f\"Rate limited. Retrying in {retry_seconds:.1f} seconds...\")\n",
    "                        time.sleep(retry_seconds)\n",
    "                        retry_count += 1\n",
    "                    else:\n",
    "                        # For non-rate-limit errors, just print and continue\n",
    "                        print(f\"Error (not retrying): {error_msg}\")\n",
    "                        break\n",
    "            \n",
    "            # Add a small delay between successful requests to avoid hitting rate limits\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Return concepts that are between 2 and 5 words long\n",
    "    return sorted([c for c in concept_set if 2 <= len(c.split()) <= 5])\n",
    "\n",
    "# Decode WNIDs to human-readable class names\n",
    "def decode_wnid(wnid):\n",
    "    synset = wn.synset_from_pos_and_offset(wnid[0], int(wnid[1:]))\n",
    "    return synset.name().split('.')[0].replace('_', ' ')\n",
    "\n",
    "def draw_red_circle(image, center, radius):\n",
    "    img = image.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    x, y = center\n",
    "    draw.ellipse((x - radius, y - radius, x + radius, y + radius), outline=\"red\", width=2)\n",
    "    return img\n",
    "\n",
    "def compute_spatial_similarity_matrix(images, concept_list, model, preprocess, device,\n",
    "                                      grid_size=(7, 7), radius=32):\n",
    "    model.eval()\n",
    "    H̃, W̃ = grid_size\n",
    "    P = torch.zeros((len(images), len(concept_list), H̃, W̃))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Precompute concept embeddings\n",
    "        text_tokens = clip.tokenize(concept_list).to(device)\n",
    "        concept_embeddings = model.encode_text(text_tokens)\n",
    "        concept_embeddings = concept_embeddings / concept_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "        for n, image in enumerate(images):\n",
    "            print(\"--\", n)\n",
    "            width, height = image.size\n",
    "            dH = height // (H̃ + 1)\n",
    "            dW = width // (W̃ + 1)\n",
    "\n",
    "            for h in range(H̃):\n",
    "                for w in range(W̃):\n",
    "                    cx = (w + 1) * dW\n",
    "                    cy = (h + 1) * dH\n",
    "                    prompted_img = draw_red_circle(image, (cx, cy), radius)\n",
    "                    input_tensor = preprocess(prompted_img).unsqueeze(0).to(device)\n",
    "\n",
    "                    image_embedding = model.encode_image(input_tensor)\n",
    "                    image_embedding = image_embedding / image_embedding.norm(dim=1, keepdim=True)\n",
    "\n",
    "                    sim = (image_embedding @ concept_embeddings.T).squeeze(0)  # (M,)\n",
    "                    P[n, :, h, w] = sim\n",
    "\n",
    "    return P  # Shape: [N, M, H̃, W̃]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e97918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder('tiny-imagenet-200/train', transform=transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = datasets.ImageFolder('tiny-imagenet-200/val', transform=transform)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "idx_to_wnid = {v: k for k, v in train_dataset.class_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f6a91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "grid_size = (7, 7)\n",
    "circle_radius = 32\n",
    "my_key = \"AIzaSyBiUwxyp8ASs_UgameBEwv5NgWUlTXLMWA\"\n",
    "genai.configure(api_key=my_key)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd7eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 200\n",
      "Exact class names: ['goldfish', 'potpie', 'hourglass', 'seashore', 'computer keyboard', 'arabian camel', 'ice cream', 'nail', 'space heater', 'cardigan', 'baboon', 'snail', 'coral reef', 'albatross', 'spider web']\n"
     ]
    }
   ],
   "source": [
    "TINY_IMAGENET_ROOT = \"tiny-imagenet-200\"\n",
    "\n",
    "# Step 1: Read wnids.txt\n",
    "with open(os.path.join(TINY_IMAGENET_ROOT, 'wnids.txt'), 'r') as f:\n",
    "    wnids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "class_start = 22\n",
    "class_end = 37\n",
    "\n",
    "print(\"Total classes:\", len(wnids))\n",
    "class_names = [decode_wnid(wnid) for wnid in wnids]\n",
    "print(\"Exact class names:\", class_names[class_start:class_end])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "935e447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concepts = generate_concept_list(class_names[class_start:class_end])  \n",
    "# print(\"Generated Concepts:\", concepts)\n",
    "\n",
    "#loading saved_concepts\n",
    "with open(\"concepts_22_37.pkl\", \"rb\") as f:\n",
    "    concepts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a5ca9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 7500 images from 15 classes.\n",
      "Collected Val 1000 images from 15 classes.\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "step2_images = []\n",
    "bottleneck_images = []\n",
    "val_image_labels = []\n",
    "counts = {wnid: 0 for wnid in wnids[class_start:class_end]}\n",
    "\n",
    "for path, label in train_dataset.samples:\n",
    "    # print(label)\n",
    "    wnid = idx_to_wnid[label]\n",
    "    if wnid in wnids[class_start:class_end] and counts[wnid] < N:\n",
    "        img = Image.open(path)\n",
    "        rgb_img = Image.open(path).convert(\"RGB\")\n",
    "        # img = pil_transform(img)\n",
    "        step2_images.append(rgb_img)\n",
    "        bottleneck_images.append((img, label))\n",
    "\n",
    "        counts[wnid] += 1\n",
    "    if all(c >= N for c in counts.values()):\n",
    "        break\n",
    "\n",
    "counts = {wnid: 0 for wnid in wnids[class_start:class_end]}\n",
    "\n",
    "for path, label in val_dataset.samples:\n",
    "    # print(label)\n",
    "    wnid = idx_to_wnid[label]\n",
    "    if wnid in wnids[class_start:class_end] and counts[wnid] < N:\n",
    "        img = Image.open(path)\n",
    "        # img = pil_transform(img)\n",
    "        val_image_labels.append((img, label))\n",
    "\n",
    "        counts[wnid] += 1\n",
    "    if all(c >= N for c in counts.values()):\n",
    "        break\n",
    "\n",
    "\n",
    "# selected_images is now a list of PIL images for the first 10 classes\n",
    "print(f\"Collected {len(step2_images)} images from {len(wnids[class_start:class_end])} classes.\")\n",
    "print(f\"Collected Val {len(val_image_labels)} images from {len(wnids[class_start:class_end])} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f17acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = compute_spatial_similarity_matrix(\n",
    "#     images=step2_images,\n",
    "#     concept_list=concepts,  \n",
    "#     model=model,\n",
    "#     preprocess=preprocess,\n",
    "#     device=device,\n",
    "#     grid_size=(7, 7),\n",
    "#     radius=32\n",
    "# )\n",
    "\n",
    "P = torch.load(\"P_22_37.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cecb2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneDataset(Dataset):\n",
    "    def __init__(self, image_label_tuples, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for image-label tuples.\n",
    "\n",
    "        Args:\n",
    "            image_label_tuples: List of tuples (PIL image, label)\n",
    "            transform: Torchvision transforms\n",
    "        \"\"\"\n",
    "        self.image_label_tuples = image_label_tuples\n",
    "        self.transform = transform if transform is not None else transforms.Compose([\n",
    "            transforms.Lambda(lambda image: image.convert(\"RGB\")),  # Ensures 3 channels\n",
    "\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_label_tuples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_label_tuples[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label)\n",
    "    \n",
    "    \n",
    "def load_backbone_model(device, num_classes=200):\n",
    "    \"\"\"\n",
    "    Load a pre-trained ResNet model and modify it for TinyImageNet.\n",
    "    \n",
    "    Args:\n",
    "        device: Device to load the model on\n",
    "        num_classes: Number of classes in TinyImageNet (default: 200)\n",
    "    \n",
    "    Returns:\n",
    "        model: Modified ResNet model\n",
    "    \"\"\"\n",
    "    # Load pre-trained ResNet18\n",
    "    backbone = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    backbone.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
    "    \n",
    "    # Move model to device\n",
    "    backbone = backbone.to(device)\n",
    "    \n",
    "    return backbone\n",
    "\n",
    "def extract_backbone_features(backbone, dataloader, device):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create a hook to capture the features\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    backbone.layer4.register_forward_hook(get_activation('layer4'))\n",
    "    \n",
    "    backbone.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, image_labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            backbone(images)  # Forward pass to trigger the hook\n",
    "            \n",
    "            # Store the features and labels\n",
    "            features.append(activation['layer4'].cpu())\n",
    "            labels.append(image_labels.cpu())\n",
    "    \n",
    "    return torch.cat(features), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb81d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BottleNeckDataset(Dataset):\n",
    "    def __init__(self, features, target_conept_maps):\n",
    "\n",
    "        self.features = features\n",
    "        self.target_conept_maps = target_conept_maps\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.features[idx], self.target_conept_maps[idx] \n",
    "\n",
    "# Define the Spatial Concept Bottleneck Layer\n",
    "class SpatialConceptBottleneckLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_concepts, grid_size=(7, 7)):\n",
    "        super(SpatialConceptBottleneckLayer, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # 1x1 convolution to project features to concept maps\n",
    "        self.bottleneck = nn.Conv2d(in_channels, num_concepts, kernel_size=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Resize features to match grid size\n",
    "        resized_features = F.interpolate(features, size=self.grid_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Apply 1x1 convolution\n",
    "        concept_maps = self.bottleneck(resized_features)  # Shape: [N, M, H̃, W̃]\n",
    "        \n",
    "        return concept_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2884a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseLinearClassifierDataset(Dataset):\n",
    "    def __init__(self, concept_maps, target_classes):\n",
    "\n",
    "        self.concept_maps = concept_maps\n",
    "        self.target_classes = target_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.concept_maps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.concept_maps[idx], self.target_classes[idx] \n",
    "\n",
    "class SparseLinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def elastic_net_regularization(W, alpha=0.5):\n",
    "    frob_norm = torch.norm(W, p='fro')**2\n",
    "    l1_norm = torch.norm(W, p=1)\n",
    "    return (1 - alpha) / 2 * frob_norm + alpha * l1_norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68aca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cubic Cosine Similarity Loss\n",
    "class CubicCosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CubicCosineSimilarityLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, concept_maps, target_similarities):\n",
    "        N, M, H, W = concept_maps.shape\n",
    "        loss = 0.0\n",
    "        \n",
    "        for m in range(M):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    # Extract q[m,h,w] and p[m,h,w] as described in the paper\n",
    "                    q = concept_maps[:, m, h, w]  # C[:, m, h, w] shape: [N]\n",
    "                    p = target_similarities[:, m, h, w]  # P[:, m, h, w] shape: [N]\n",
    "                    \n",
    "                    # Zero-mean normalization\n",
    "                    q_norm = q - q.mean()\n",
    "                    p_norm = p - p.mean()\n",
    "                    \n",
    "                    # Cubic transformation (raise to power of 3)\n",
    "                    q_cubic = q_norm ** 3\n",
    "                    p_cubic = p_norm ** 3\n",
    "                    \n",
    "                    # L2 normalization\n",
    "                    q_cubic_norm = q_cubic / (torch.norm(q_cubic) + 1e-8)\n",
    "                    p_cubic_norm = p_cubic / (torch.norm(p_cubic) + 1e-8)\n",
    "                    \n",
    "                    # Cosine similarity\n",
    "                    sim = torch.dot(q_cubic_norm, p_cubic_norm)\n",
    "                    \n",
    "                    # Negative similarity for loss minimization\n",
    "                    loss -= sim\n",
    "        \n",
    "        return loss / (M * H * W)\n",
    "\n",
    "\n",
    "def train_concept_bottleneck_layer(features, labels, P, device, grid_size=(7, 7), num_epochs=10):\n",
    "    # step 3\n",
    "\n",
    "    in_channels = features.shape[1]  # Channels in backbone features\n",
    "    bottleneck = SpatialConceptBottleneckLayer(in_channels, len(concepts), grid_size).to(device)\n",
    "\n",
    "\n",
    "    criterion = CubicCosineSimilarityLoss()\n",
    "    optimizer = optim.Adam(bottleneck.parameters(), lr=0.001)\n",
    "\n",
    "    # print(features.shape)\n",
    "    # features = features.to(device)\n",
    "    # P = P.to(device)\n",
    "\n",
    "    bottleneck_dataset = BottleNeckDataset(features=features, target_conept_maps=P)\n",
    "    bottleneck_dataloader = DataLoader(bottleneck_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    bottleneck.train()\n",
    "    print(\"training bottleneck\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_features, target_concept_maps in  tqdm(bottleneck_dataloader):\n",
    "            batch_features = batch_features.to(device)\n",
    "            target_concept_maps = target_concept_maps.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_concept_maps = bottleneck(batch_features)\n",
    "            loss = criterion(output_concept_maps, target_concept_maps)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_features.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(bottleneck_dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # step 4\n",
    "    all_concept_maps_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_features, target_concept_maps in  bottleneck_dataloader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            concept_maps = bottleneck(batch_features)  \n",
    "            all_concept_maps_list.append(concept_maps.cpu())  \n",
    "\n",
    "    all_concept_maps = torch.cat(all_concept_maps_list, dim=0)  # \n",
    "\n",
    "    print(f\"Generated concept maps shape: {all_concept_maps.shape}\")\n",
    "    \n",
    "    concept_activations = F.adaptive_avg_pool2d(concept_maps, 1).squeeze(-1).squeeze(-1)  \n",
    "    mapped_labels = labels - 22\n",
    "\n",
    "    print(mapped_labels.shape)\n",
    "\n",
    "    classifier_head_dataset = SparseLinearClassifierDataset(concept_activations, mapped_labels)\n",
    "    classifier_head_dataloader = DataLoader(classifier_head_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    classifier_head = SparseLinearClassifier(input_dim=all_concept_maps.shape[1], num_classes=15).to(device)\n",
    "    classifier_head_optimizer = torch.optim.SGD(classifier_head.parameters(), lr=0.01)  # placeholder for GLM-SAGA\n",
    "    lambda_reg = 1e-4\n",
    "    alpha = 0.5\n",
    "    classifier_head_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    classifier_head.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for x_concepts, x_concepts_label in tqdm(classifier_head_dataloader):\n",
    "            x_concepts, x_concepts_label = x_concepts.to(device), x_concepts_label.to(device)\n",
    "            classifier_head_optimizer.zero_grad()\n",
    "\n",
    "            logits = classifier_head(x_concepts)  # shape: [batch_size, num_classes]\n",
    "            loss_ce = classifier_head_criterion(logits, x_concepts_label)\n",
    "\n",
    "            W = classifier_head.linear.weight\n",
    "            loss_reg = lambda_reg * elastic_net_regularization(W, alpha)\n",
    "            loss = loss_ce + loss_reg\n",
    "\n",
    "            loss.backward()\n",
    "            classifier_head_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_concepts.size(0)\n",
    "\n",
    "            # Accuracy computation\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == x_concepts_label).sum().item()\n",
    "            total += x_concepts_label.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(classifier_head_dataloader.dataset)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        return bottleneck, classifier_head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  93%|█████████▎| 55/59 [02:28<00:10,  2.52s/it]"
     ]
    }
   ],
   "source": [
    "backbone = load_backbone_model(device)\n",
    "backbone.eval() \n",
    "backbone_data = BackboneDataset(bottleneck_images)\n",
    "backbone_dataloader = DataLoader(backbone_data, batch_size=128, shuffle=False)\n",
    "features, labels = extract_backbone_features(backbone, backbone_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3edf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [02:32<00:00,  2.58s/it]\n",
      "100%|██████████| 1/1 [02:32<00:00, 152.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: -0.2338\n",
      "Generated concept maps shape: torch.Size([7500, 134, 7, 7])\n",
      "torch.Size([7500])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m grid_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m) \n\u001b[1;32m      2\u001b[0m num_concepts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(concepts)\n\u001b[0;32m----> 3\u001b[0m bottleneck, classifier_head \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_concept_bottleneck_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 96\u001b[0m, in \u001b[0;36mtrain_concept_bottleneck_layer\u001b[0;34m(backbone, dataloader, P, device, grid_size, num_epochs)\u001b[0m\n\u001b[1;32m     93\u001b[0m classifier_head_dataset \u001b[38;5;241m=\u001b[39m SparseLinearClassifierDataset(concept_activations, mapped_labels)\n\u001b[1;32m     94\u001b[0m classifier_head_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(classifier_head_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 96\u001b[0m classifier_head \u001b[38;5;241m=\u001b[39m \u001b[43mSparseLinearClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_concept_maps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     97\u001b[0m classifier_head_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(classifier_head\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)  \u001b[38;5;66;03m# placeholder for GLM-SAGA\u001b[39;00m\n\u001b[1;32m     98\u001b[0m lambda_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m, in \u001b[0;36mSparseLinearClassifier.__init__\u001b[0;34m(self, input_dim, num_classes)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, num_classes):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Academics/Sem 8/XAI/XAI-Project/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:106\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\""
     ]
    }
   ],
   "source": [
    "grid_size = (7, 7) \n",
    "num_concepts = len(concepts)\n",
    "bottleneck, classifier_head = train_concept_bottleneck_layer(\n",
    "    features=features,\n",
    "    labels = labels,\n",
    "    dataloader=backbone_dataloader, \n",
    "    P=P,  \n",
    "    device=device,\n",
    "    grid_size=(7, 7), \n",
    "    num_epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7aa25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is incomplete\n",
    "def predict_class(backbone, bottleneck, classifier_head, image, preprocess, device):\n",
    "    backbone.eval()\n",
    "    bottleneck.eval()\n",
    "    classifier_head.eval()\n",
    "    \n",
    "    # Create a hook to capture the features\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register the hook\n",
    "    backbone.layer4.register_forward_hook(get_activation('layer4'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Preprocess and forward through backbone\n",
    "        input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        backbone(input_tensor)\n",
    "        \n",
    "        # Extract features and predict concept maps\n",
    "        features = activation['layer4']\n",
    "        concept_maps = bottleneck(features)\n",
    "        concept_activation = F.adaptive_avg_pool2d(concept_maps, 1).squeeze(-1).squeeze(-1)  \n",
    "        logits = classifier_head(concept_activation) \n",
    "        prediction = torch.argmax(logits, dim=1)\n",
    "    return prediction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
